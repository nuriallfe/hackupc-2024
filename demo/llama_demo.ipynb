{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, StorageContext, ServiceContext\n",
    "from llama_index.indices.vector_store import VectorStoreIndex\n",
    "from llama_iris import IRISVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: db8290fd-7d79-45e3-a663-0927be342542\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"../data/apunts\").load_data()\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'demo'\n",
    "password = 'demo' \n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972' \n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = IRISVectorStore.from_params(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    table_name=\"apunts\",\n",
    "    embed_dim=1536,  # openai embedding dimension\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding()\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     embed_model=embed_model, llm=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 945.09it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context, \n",
    "    show_progress=True, \n",
    "    # service_context=service_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If reconnecting to the vector store, use this: \n",
    "\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# query_engine = index.as_query_engine()\n",
    "\n",
    "# # Adding documents to existing index\n",
    "\n",
    "# for d in documents:\n",
    "#     index.insert(document=d, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG is a technique used to base the responses of a Language Model on validated external information.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mid-1980s saw the emergence of a significant technological advancement in the form of Optical\n",
      "Character Recognition (OCR) technology. This technology played a crucial role in the processing of\n",
      "PDF documents with tables, as it enabled the extraction of textual information from visual elements\n",
      "within these documents.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What happened in the mid 1980s?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the purpose of the RAG technique in relation to a LLM? - The RAG technique is used to base\n",
      "the responses of a LLM on validated external information.  How can contextual integration be applied\n",
      "in RAG? - Contextual integration in RAG involves loading context into a database, which is then\n",
      "loaded to be inserted into a prompt.  What are some steps involved in the RAG technique? - Obtaining\n",
      "textual content from documents, extracting elements, handling metadata such as name and type.  How\n",
      "are documents normalized in the context provided? - Documents are normalized by converting them into\n",
      "a common format to identify common elements, enabling consistent preprocessing and reducing\n",
      "processing costs.  What are some examples of different document formats mentioned in the context? -\n",
      "JSON, HTML, PowerPoint, and PDF with tables are mentioned as different document formats.  How is\n",
      "semantic search utilized in the context of document processing? - Semantic search involves using a\n",
      "database of vectors representing documents to find semantically similar content, often using\n",
      "embeddings to convert text to vectors.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Create questions, and answers, based on the info you know\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
